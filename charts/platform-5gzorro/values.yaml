kub-prometheus-stack:
  enabled: false
  defaultRules:
    etcd: false
    kubeScheduler: false
  grafana:
    defaultDashboardsTimezone: Europe/Rome
    adminPassword: 5gzorro-metrics
  kubeControllerManager:
    enabled: false
  kubeEtcd:
    enabled: false
  kubeScheduler:
    enabled: false
  prometheus:
    prometheusSpec:
      serviceMonitorSelectorNilUsesHelmValues: false
      retention: 180d
      retentionSize: "30GB"
      storageSpec:
        volumeClaimTemplate:
          spec:
            storageClassName: managed-premium
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 32Gi

loki-stack:
  enabled: false
  loki:
    enabled: true
    isDefault: true
    persistence:
      enabled: true
      accessModes:
      - ReadWriteOnce
      size: 32Gi
      annotations: {}
      subPath: "loki"
      storageClassName: managed-premium
    serviceMonitor:
      enabled: false
      prometheusRule:
        enabled: false
        additionalLabels: {}
        rules: 
        #  Some examples from https://awesome-prometheus-alerts.grep.to/rules.html#loki
        - alert: LokiProcessTooManyRestarts
          expr: changes(process_start_time_seconds{job=~"loki"}[15m]) > 2
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Loki process too many restarts (instance {{ $labels.instance }})
            description: "A loki process had too many restarts (target {{ $labels.instance }})\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
        - alert: LokiRequestErrors
          expr: 100 * sum(rate(loki_request_duration_seconds_count{status_code=~"5.."}[1m])) by (namespace, job, route) / sum(rate(loki_request_duration_seconds_count[1m])) by (namespace, job, route) > 10
          for: 15m
          labels:
            severity: critical
          annotations:
            summary: Loki request errors (instance {{ $labels.instance }})
            description: "The {{ $labels.job }} and {{ $labels.route }} are experiencing errors\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
        - alert: LokiRequestPanic
          expr: sum(increase(loki_panic_total[10m])) by (namespace, job) > 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: Loki request panic (instance {{ $labels.instance }})
            description: "The {{ $labels.job }} is experiencing {{ printf \"%.2f\" $value }}% increase of panics\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
        - alert: LokiRequestLatency
          expr: (histogram_quantile(0.99, sum(rate(loki_request_duration_seconds_bucket{route!~"(?i).*tail.*"}[5m])) by (le)))  > 1
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: Loki request latency (instance {{ $labels.instance }})
            description: "The {{ $labels.job }} {{ $labels.route }} is experiencing {{ printf \"%.2f\" $value }}s 99th percentile latency\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
  promtail:
    enabled: true
    config:
      lokiAddress: http://{{ .Release.Name }}-loki:3100/loki/api/v1/push
    serviceMonitor:
      enabled: false

minio:
  enabled: true
  fullnameOverride: "minio"
  mode: standalone
  replicas: 1
  DeploymentUpdate:
    type: Recreate
  rootUser: ""
  rootPassword: ""
  resources:
    requests:
      memory: 2Gi
  persistence:
    enabled: true
    storageClass: ""
    accessModes: "ReadWriteOnce"
    size: 32Gi
  metrics:
    serviceMonitor:
      enabled: false

postgresOperator:
  enabled: true
  postgresqlClusters:
    - name: zorro5g-global-db
      spec:
        teamId: 5gzorro
        volume:
          size: 1Gi
          storageClass: ""
        numberOfInstances: 1
        users:
          zorro5g:
            - superuser
            - createdb
        databases:
          global: zorro5g
        postgresql:
          version: "14"

strimzi:
  enabled: true
  watchAnyNamespace: true
  kafkaClusters:
    - name: kafka-cluster
      replicas: 3
      version: 3.2.0
      config:
        offsets.topic.replication.factor: 3
        transaction.state.log.replication.factor: 3
        transaction.state.log.min.isr: 2
        default.replication.factor: 3
        min.insync.replicas: 1
        inter.broker.protocol.version: "3.2"
      storage:
        type: jbod
        volumes:
        - id: 0
          type: persistent-claim
          size: 32Gi
          deleteClaim: false
        - id: 1
          type: persistent-claim
          size: 32Gi
          deleteClaim: false
      zookeeper:
        replicas: 3
        storage:
          type: persistent-claim
          size: 32Gi
  kafkaTopics:
    - name: test1
      clusterName: kafka-cluster
      spec:
        partitions: 3
        replicas: 1
    - name: test2
      clusterName: kafka-cluster
      spec:
        partitions: 3
        replicas: 1
